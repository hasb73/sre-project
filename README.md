
# Azure Multi-Region Disaster Recovery Solution

<img width="2803" height="2501" alt="image" src="https://github.com/user-attachments/assets/9c27ead6-d9d2-47b4-abd0-7a4d00da2a60" />



The setup uses Azure with AKS and supporting services to deploy a HA and DR setup for JupyterHub and a stateless microservices implementation. The primary goals are high availability and resiliency in case of a regional outage with minimal RPO and RTO.

## Core Components

### Infrastructure as Code: Terraform

Terraform is used to provision the shared resources and the primary and DR environments with re-usable modules.

- **terraform/shared**: This module provisions the storage accounts, traffic manager and VNet peering. Initially this module is deployed first before the primary and DR environments can be provisioned.

- **terraform/modules/**: This contains the re-usable modules used to deploy the AKS cluster, networking, storage, JupyterHub (via Helm), application gateway and Key Vault with its variables and outputs. The benefit here is that any environment under terraform/environments can be deployed with consistency by calling these modules and supplying the environment-specific variables.

- **terraform/environments**: This contains the primary and DR environments. The primary is deployed in UAE North and DR is deployed in EU North with all the infra declared in the shared modules.

### Storage

Azure Storage accounts (primaryst01 - UAE North and drst01 - EU North) are provisioned by the shared layer. The storage accounts have the following use cases and purposes:

- Store the Terraform state file
- Store the file shares (jupyterhub-users) generated by the JupyterHub users which contains the user data such as notebooks
- The primaryst01 is provisioned with GRS (Geo Replicated Storage) to UAE Central
- File sync between the file shares happens via cron jobs in AKS

### Network

Two VNets are provisioned:
- **primary-vnet**: 10.1.0.0/16 CIDR
- **dr-vnet**: 10.2.0.0/16 CIDR
- NSG rules allowing PostgreSQL traffic (port 5432)

VNet peering takes place between the two to ensure PostgreSQL database replication can happen seamlessly. Additionally, subnets are present in each VNet for AKS, Application Gateway, and database.

### AKS

Two AKS clusters are provisioned with the below components:

- The AKS node pool is provisioned with `Standard_D2s_v3` node type using AzureLinux
- Isolated namespaces: database, app and jupyterhub with network policies
- Application Gateway Ingress Controller (AGIC) addon
- CSI driver setup for Key Vault secrets storage
- Network policies to allow ingress and egress communication between the app/jupyterhub and database namespace
- Each namespace has its deployments, services, ingresses and secrets management with SecretProviderClass
- AKS is configured with Azure Log Analytics for historical log viewing and querying with KQL

### Application Gateway

Two application gateways with public IPs are provisioned by Terraform:
- **primary-appgw** (Primary region)
- **dr-appgw** (DR region)

The ingresses created within the namespaces get registered to the application gateway load balancers.

### Traffic Manager

Two Azure Traffic Manager profiles are setup:
- **jupyterhub**: http://jupyterhub.trafficmanager.net/
- **microservice**: http://microservice.trafficmanager.net/

Both use priority routing between the primary (priority 1) and DR (priority 2) Application Gateway public IPs with health probes enabled.

### Azure Key Vault

Azure Key Vault is created by Terraform in both regions:
- **kv-sreproject-primary-01** (Primary region)
- **kv-sreproject-dr-01** (DR region)

The AKS interacts with the Key Vault using the CSI driver and the `secrets-store-csi-driver`. Each namespace has its SecretProviderClass which defines the secrets to be used.

### Azure Container Registry (ACR)

An Azure Container Registry (ACR) is created in the primary region: **sreproject01**

The microservices app builds and pushes to this ACR and the deployments pull from the ACR using the image pull secrets.

## Deployment

### How are the infra and services deployed?

The above components are deployed and setup via the primary and DR scripts in `/scripts/`:
- `deploy_primary.sh`
- `deploy_dr.sh`

These scripts use Terraform to deploy the underlying infrastructure with the modules and then call upon additional utilities such as shell scripts and Kustomize to populate and setup the namespaces. This is to ensure maintainability.

## Functionality

At a high level, the setup achieves HA and performs DR as follows:

- PostgreSQL is deployed to both clusters via StatefulSets and setup for replication to the secondary database
- An init container sets up the PostgreSQL in the DR cluster as the secondary
- JupyterHub is deployed in primary and secondary using the Helm charts and configured to use PostgreSQL
- JupyterHub user logins, metadata, tokens are stored in PostgreSQL and replicated with replication lag under 5s
- Microservices (frontend-api, business-logic and data-ingest) are deployed to both clusters and use PostgreSQL for storing transactions
- On the primary AKS cluster, cron jobs (15 minutes frequency) using AzCopy copy the user files to the DR file share
- The hub and microservices pods are in a scaled down state on the DR cluster

## Failover and Failback

### Failover Process

In the event of a failover, the `scripts/failover_full_stack.sh` is initiated by the administrator:

1. The script scales down the application and JupyterHub in the primary
2. The Traffic Manager primary endpoints go into degraded state due to health check failures
3. It then triggers an on-demand job (from the `cronjob/azure-files-sync`) to trigger a file sync between the primary and DR shares
4. Subsequently the script logs into the DR cluster, runs PostgreSQL replication checks and performs promote using `pg_promote`
5. The JupyterHub and microservices pods are scaled up in the DR cluster and connect to PostgreSQL
6. The Traffic Manager DR endpoints are now online. A user can login using http://jupyterhub.trafficmanager.net/

### Failback Process

Once the primary is functional, we trigger the `scripts/failback_full_stack.sh` and a sync is performed for the files created during the duration JupyterHub was failed over.


## Network Info

**Primary Region (UAE North):**
- VNet: 10.1.0.0/16
  - AKS Subnet: 10.1.0.0/20
  - Database Subnet: 10.1.16.0/24
  - Services Subnet: 10.1.17.0/24

**DR Region (North Europe):**
- VNet: 10.2.0.0/16
  - AKS Subnet: 10.2.0.0/20
  - Database Subnet: 10.2.16.0/24
  - Services Subnet: 10.2.17.0/24

**Cross-Region Connectivity:**
- VNet Peering for database replication
- NSG rules allowing PostgreSQL traffic (port 5432)
- Traffic Manager for DNS failover




##  Namespace Info

The solution uses separate Kubernetes namespaces for better isolation and security:

| Namespace | Purpose | Resources |
|-----------|---------|-----------|
| `database` | PostgreSQL databases | Primary/Secondary StatefulSets, Services, Secrets |
| `default` | Microservices | Frontend API, Business Logic, Data Ingest |
| `jupyterhub` | JupyterHub | Hub, Proxy, User Pods |


### Service DNS Names

**Primary Region:**
- Database: `postgresql.database.svc.cluster.local:5432`
- Frontend API: `frontend-api.default.svc.cluster.local:8080`
- JupyterHub: `proxy-public.jupyterhub.svc.cluster.local:80`

**DR Region:**
- Database: `postgresql-secondary.database.svc.cluster.local:5432`
- Frontend API: `frontend-api.default.svc.cluster.local:8080`
- JupyterHub: `proxy-public.jupyterhub.svc.cluster.local:80`

## Deploy infra

### Initialize Terraform Backend

Create Azure Storage for Terraform state:

```bash
# Create resource group
az group create --name terraform-state-rg --location uaenorth

# Create storage account (name must be globally unique)
az storage account create \
  --name tfstateazuredr \
  --resource-group terraform-state-rg \
  --location uaenorth \
  --sku Standard_LRS \
  --encryption-services blob

# Create container
az storage container create \
  --name tfstate \
  --account-name tfstateazuredr

# Get storage account key
az storage account keys list \
  --resource-group terraform-state-rg \
  --account-name tfstateazuredr \
  --query '[0].value' -o tsv
```


### Deploy Infrastructure and Namespaces

These scripts use Terraform to deploy the underlying infrastructure with the modules and then call upon additional utilities such as shell scripts and Kustomize to populate and setup the namespaces. This is to ensure maintainability.


```bash
# Deploy primary region 
./scripts/deploy/deploy_primary.sh

# Deploy DR region
./scripts/deploy/deploy_dr.sh
```

### Application Components

**JupyterHub**:
- Multi-user notebook environment
- Persistent user storage with Azure File Shares

**Microservices** (Python Flask):
- **Frontend API** (Port 8080): User-facing API gateway
- **Business Logic** (Port 8081): Core business operations
- **Data Ingest** (Port 8082): Data ingestion and processing



**Build and Push Microservices Container Images:**

```bash
cd microservices

# Login to container registry
az acr login --name sreproject01

# Set environment variables
export CONTAINER_REGISTRY="sreproject01.azurecr.io"
export VERSION="1.1."

# Build and push all services
./build-and-push.sh
```



### Deploy JupyterHub (Created by terraform+helm but steps are here)

**Primary Region:**

```bash
kubectl config use-context primary

# Add JupyterHub Helm repository
helm repo add jupyterhub https://hub.jupyter.org/helm-chart/
helm repo update

# Deploy JupyterHub
helm install jupyterhub jupyterhub/jupyterhub \
  --namespace jupyterhub \
  --create-namespace \
  --values kubernetes/jupyterhub/values-primary.yaml \
  --version 4.3.1

# Wait for deployment
kubectl wait --for=condition=ready pod -l component=hub -n jupyterhub --timeout=600s
```



### Verify Database Replication**

```bash
# Run verification script
./kubernetes/postgresql/scripts/verify_replication.sh

# Expected output:
# Replication Status: OK
# Primary LSN: 0/3000000
# Secondary LSN: 0/3000000
# Lag: 0.5 seconds
```



###  Failover Procedure

**Execute Failover**

**Usage:**
```bash
./scripts/failover/failover_full_stack.sh
```

**What it does:**
1. Connects to Primary cluster
2. Scales down JupyterHub hub (stops accepting new users)
3. Triggers Azure Files delta sync (Primary → DR)
4. Connects to DR cluster
5. Checks DR database pod status
6. Verifies database recovery status
7. Promotes DR database to primary
8. Scales up JupyterHub hub and application in DR
9. Verifies pod health and readiness

**RTO:** ~2-5 minutes for the promote and pods to scale

**RPO:** ~15 minutes at the most due to the scheduled file sync cron job,immediate if the file sync runs as part of the script


###  Failback Procedure

**Usage:**
```bash
./scripts/failover/failback_full_stack.sh
```

**What it does:**
1. Connects to DR cluster
2. Scales down JupyterHub hub in DR
3. Triggers Azure Files delta sync (DR → Primary)
4. Demotes DR database back to replica
5. Connects to Primary cluster
6. Checks Primary database pod status
7. Verifies Primary database is in primary mode
8. Verifies replication from Primary to DR
9. Scales up JupyterHub hub in Primary
10. Verifies pod health and readiness

**Duration:** ~2-5 minutes



## Teardown Instructions

### Automated Cleanup

```bash
# Preview resources to be deleted
./scripts/cleanup/cleanup_primary.sh --dry-run
./scripts/cleanup/cleanup_dr.sh --dry-run

```


## Recovery Objectives

### RPO (Recovery Point Objective)

**Target: < 15 minutes**

- Achieved through PostgreSQL streaming replication
- Replication lag typically is below 1 second
- The azcopy cron jobs runs every 15minutes, but the failover script does an ondemand copy so the RPO can be under ~2 minutes

### RTO (Recovery Time Objective)

**Target: < 5 minutes**

**Breakdown:**
- Detection: 1-2 minutes
- Script execution (failover, pod readines) - 2-3 minutes
- Traffic Manager DNS Propagation: ~ 1 minute (TTL is 10s and health probe is 10s)


## Root Directory

```
sre-project/
├── app/                          # Microservices application code
├── kubernetes/                   # Kubernetes manifests and configurations
├── scripts/                      # Automation scripts for deployment and operations
├── terraform/                    # Infrastructure as Code (Terraform)
├── README.md                     # Main project documentation
└── REFLECTIONS.md               # Project reflections and learnings
```

---

## Application Directory (`app/`)

Contains the source code for three Python Flask microservices.

```
app/
├── business-logic/              # Business logic microservice
│   ├── app.py                   # Flask application for business operations
│   ├── Dockerfile               # Container image definition
│   └── requirements.txt         # Python dependencies
│
├── data-ingest/                 # Data ingestion microservice
│   ├── app.py                   # Flask application for data processing
│   ├── Dockerfile               # Container image definition
│   └── requirements.txt         # Python dependencies
│
├── frontend-api/                # Frontend API gateway
│   ├── app.py                   # Flask application for API endpoints
│   ├── Dockerfile               # Container image definition
│   └── requirements.txt         # Python dependencies
│
├── build-and-push.sh            # Script to build and push all images to ACR
├── init-db.sql                  # Database initialization SQL script
└── README.md                    # Application documentation
```

---

## Kubernetes Directory (`kubernetes/`)

Contains all Kubernetes manifests organized by component and environment.

### JupyterHub (`kubernetes/jupyterhub/`)

```
kubernetes/jupyterhub/
├── primary/                     # Primary region JupyterHub resources
│   ├── azcopy-sync-cronjob.yaml        # CronJob for syncing user files to DR
│   ├── azure-files-pv-primary.yaml     # PersistentVolume for user storage
│   ├── jupyterhub-ingress.yaml         # Ingress configuration
│   └── secret-provider-class-primary.yaml  # Key Vault CSI driver config
│
├── dr/                          # DR region JupyterHub resources
│   ├── azcopy-dr-to-primary-job.yaml   # Job for failback file sync
│   ├── azure-files-pv-dr.yaml          # PersistentVolume for DR storage
│   └── secret-provider-class-dr.yaml   # Key Vault CSI driver config for DR
│
├── scripts/                     # JupyterHub utility scripts
│   ├── azcopy-secret-generation.sh     # Generate AzCopy SAS tokens
│   └── verify-database-replication.sh  # Verify JupyterHub DB replication
│
├── values-primary.yaml          # Helm values for primary JupyterHub
├── values-dr.yaml               # Helm values for DR JupyterHub
└── README.md                    # JupyterHub deployment documentation
```

### Microservices (`kubernetes/microservices/`)

```
kubernetes/microservices/
├── primary/                     # Primary region microservices
│   ├── business-logic.yaml      # Business logic deployment and service
│   ├── data-ingest.yaml         # Data ingest deployment and service
│   ├── frontend-api.yaml        # Frontend API deployment and service
│   ├── configmap.yaml           # Environment configuration
│   ├── kustomization.yaml       # Kustomize configuration
│   └── microservices-ingress.yaml  # Ingress for microservices
│
├── dr/                          # DR region microservices
│   ├── business-logic.yaml      # Business logic deployment (scaled down)
│   ├── data-ingest.yaml         # Data ingest deployment (scaled down)
│   ├── frontend-api.yaml        # Frontend API deployment (scaled down)
│   ├── configmap.yaml           # DR environment configuration
│   ├── kustomization.yaml       # Kustomize configuration for DR
│   └── microservices-ingress.yaml  # Ingress for DR microservices
│
└── scripts/                     # Microservices utility scripts
    ├── setup-app-namespace.sh   # Create and configure app namespace
    ├── test-app.sh              # Test microservices endpoints
    └── test-database.sh         # Test database connectivity
```

### PostgreSQL (`kubernetes/postgresql/`)

```
kubernetes/postgresql/
├── primary/                     # Primary PostgreSQL database
│   ├── statefulset.yaml         # Primary database StatefulSet
│   ├── service.yaml             # Service for primary database
│   ├── configmap.yaml           # PostgreSQL configuration
│   ├── init-configmap.yaml      # Initialization scripts
│   ├── namespace.yaml           # Database namespace definition
│   ├── network-policy.yaml      # Network policies for database access
│   ├── primary-external-service.yaml  # External service for replication
│   ├── secret-provider-class.yaml     # Key Vault integration
│   └── kustomization.yaml       # Kustomize configuration
│
├── secondary/                   # Secondary PostgreSQL database (DR)
│   ├── statefulset.yaml         # Secondary database StatefulSet with replication
│   ├── service.yaml             # Service for secondary database
│   ├── configmap.yaml           # PostgreSQL configuration for standby
│   ├── namespace.yaml           # Database namespace for DR
│   ├── secret-provider-class.yaml     # Key Vault integration for DR
│   └── kustomization.yaml       # Kustomize configuration for DR
│
├── backup/                      # Database backup configurations
│   ├── backup-to-azure-cronjob.yaml   # CronJob for automated backups
│   ├── restore-backup.sh        # Script to restore from backup
│   └── setup-backup.sh          # Setup backup infrastructure
│
├── scripts/                     # PostgreSQL utility scripts
│   └── verify_replication.sh    # Verify streaming replication status
│
└── README.md                    # PostgreSQL deployment documentation
```

### Other Kubernetes Files

```
kubernetes/
└── setup-appgw.sh               # Script to setup Application Gateway AGIC
```

---

## Scripts Directory (`scripts/`)

Automation scripts for deployment, cleanup, and disaster recovery operations.

```
scripts/
├── deploy/                      # Deployment scripts
│   ├── deploy_primary.sh        # Deploy primary region (with Terraform dry run)
│   └── deploy_dr.sh             # Deploy DR region (with Terraform dry run)
│
├── cleanup/                     # Cleanup scripts
│   ├── cleanup_primary.sh       # Cleanup primary region (with destroy dry run)
│   └── cleanup_dr.sh            # Cleanup DR region (with destroy dry run)
│
└── failover/                    # Disaster recovery scripts
    ├── failover_full_stack.sh   # Failover from primary to DR
    ├── failback_full_stack.sh   # Failback from DR to primary
    └── README.md                # Failover procedures documentation
```
